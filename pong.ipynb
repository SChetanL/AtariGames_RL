{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7mJmWgJerG"
      },
      "source": [
        "# II. Problem\n",
        "\n",
        "\n",
        "## Pong\n",
        "\n",
        "Pong is one of the simplest Atari game as in the figure below. The goal of the game is winning the pingpong/tennis-like game by scoring 21 first. A player scores one point when the opponent hits the ball out of bounds or misses a hit. The right paddle is controlled by your RL agent and left paddle is controlled by a computer.\n",
        "\n",
        "![pong](https://ale.farama.org/_static/videos/environments/pong.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr6wAbHcJerH",
        "outputId": "a7578603-6f08-4e43-861a-e82efa2703d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /opt/anaconda3/lib/python3.12/site-packages (0.29.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KBl-qggHJerI"
      },
      "outputs": [],
      "source": [
        "## import basic libraries:\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib as mpl\n",
        "import PIL.Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from functools import partial\n",
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "from tf_agents.environments import suite_gym, suite_atari\n",
        "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
        "from tf_agents.environments.atari_wrappers import FrameStack4\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.eval.metric_utils import log_metrics\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.trajectories.trajectory import to_transition\n",
        "from tf_agents.utils.common import function\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.networks.q_network import QNetwork\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giOpOSXCJerI"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X-ZjgowJerJ"
      },
      "outputs": [],
      "source": [
        "# stable and repeatable runs:\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# load the atari environment\n",
        "max_episode_frames = 108000\n",
        "env_name = \"PongNoFrameskip-v4\"\n",
        "\n",
        "env = suite_atari.load(\n",
        "    env_name,\n",
        "    max_episode_steps=max_episode_frames/4,\n",
        "    gym_env_wrappers=[AtariPreprocessing, FrameStack4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hzDDZlgJerJ",
        "outputId": "48bb5355-1cc2-4ed3-d84b-a3c85467003d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACh0lEQVR4nO3bPUoDURhAUSMD2lu4CDdgaZeVWNq6GVeSzjIbcBEp0sfOJsg0EgPODLmeUwXyMw8uHw/mTVZXP3h5ffjpLS7Ias6Qz0+nr/X2/jHDSuZx+Nyc/MztzXrSNVxP+ussTuA4geOGpS483mt/szdfuvFe+5u9+a+Y4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOW+xe9H+4/zw25/3nMRMcJ3CcwHGzPpPF/ExwnMBxw3a3X3oNTMgExwkcJ3CcwHECxwkcJ3CcwHECxw2P93dLr4EJmeA4geMEjhM4TuA4geMEjhM4TuA4geOOgbe7vafvkkxwnMBxAscd/x/s0LDKBMcJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHDcsvQBOO3xuvl/f3qzP+q4JjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA458EX4Nwz4DETHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHDdsd/ul18CETHCcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECx30Bu1ci6JJ73gAAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=160x210 at 0x7F1CDC042B10>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rvKfgQcJerK"
      },
      "outputs": [],
      "source": [
        "# to make the environment usable from within TF graph\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "tf_env = TFPyEnvironment(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rFQRuhiJerK"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-hyAC1sJerK"
      },
      "outputs": [],
      "source": [
        "# Q-Network, DQN\n",
        "\n",
        "preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)\n",
        "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
        "fc_layer_params=[512]\n",
        "\n",
        "q_net = QNetwork(\n",
        "    tf_env.observation_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    preprocessing_layers=preprocessing_layer,\n",
        "    conv_layer_params=conv_layer_params,\n",
        "    fc_layer_params=fc_layer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8DS3tpYJerK"
      },
      "source": [
        "## Agent & epsilon-greedy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APAqYTdJJerK"
      },
      "outputs": [],
      "source": [
        "# switch the optimizer & loss function by commenting/uncommenting optimizer & td_errors_loss_fn\n",
        "\n",
        "train_step = tf.Variable(0)\n",
        "update_period = 4\n",
        "learning_rate = 2.5e-4\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate) # uncomment/comment\n",
        "#optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0, epsilon=0.00001, centered=True) # uncomment/comment\n",
        "\n",
        "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=1.0,\n",
        "    decay_steps=150000 // update_period,\n",
        "    end_learning_rate=0.01)\n",
        "\n",
        "agent = DqnAgent(tf_env.time_step_spec(),\n",
        "                 tf_env.action_spec(),\n",
        "                 q_network=q_net,\n",
        "                 optimizer=optimizer,\n",
        "                 target_update_period=2000,\n",
        "                 td_errors_loss_fn=common.element_wise_squared_loss,       # uncomment/comment\n",
        "                 #td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),  # uncomment/comment\n",
        "                 gamma=0.99,\n",
        "                 train_step_counter=train_step,\n",
        "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68QGgsiAJerL"
      },
      "source": [
        "## Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y19H70q8JerL"
      },
      "outputs": [],
      "source": [
        "\n",
        "replay_buffer_max_length = 100000\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=tf_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "replay_buffer_observer = replay_buffer.add_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU91n7i7JerL",
        "outputId": "3af78e6c-a913-46ac-cf98-d5a5a5e3fbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000/20000"
          ]
        }
      ],
      "source": [
        "# Show progress class\n",
        "\n",
        "class ShowProgress:\n",
        "    def __init__(self, total):\n",
        "        self.counter = 0\n",
        "        self.total = total\n",
        "    def __call__(self, trajectory):\n",
        "        if not trajectory.is_boundary():\n",
        "            self.counter += 1\n",
        "        if self.counter % 100 == 0:\n",
        "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n",
        "\n",
        "# fill the replay buffer with random data\n",
        "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
        "                                        tf_env.action_spec())\n",
        "init_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    initial_collect_policy,\n",
        "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
        "    num_steps=20000)\n",
        "final_time_step, final_policy_state = init_driver.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MU4JnnOJerL"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMe0BNDvJerL"
      },
      "outputs": [],
      "source": [
        "# create training metrics, using TF-agents\n",
        "\n",
        "train_metrics = [\n",
        "    tf_metrics.NumberOfEpisodes(),\n",
        "    tf_metrics.EnvironmentSteps(),\n",
        "    tf_metrics.AverageReturnMetric(),\n",
        "    tf_metrics.AverageEpisodeLengthMetric(),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV7OvRjyJerL"
      },
      "source": [
        "## Collect Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v1PAS2eJerL"
      },
      "outputs": [],
      "source": [
        "#Create the Collect Driver, passes current time step to the collect policy, which returns the action step\n",
        "\n",
        "collect_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer_observer] + train_metrics,\n",
        "    num_steps=update_period)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dipgxgPuJerL"
      },
      "source": [
        "## Create the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzc8K4WhJerL",
        "outputId": "6b69514c-9f88-4e5b-9912-2a1596f293df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/riku/miniconda3/envs/py379/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ],
      "source": [
        "# create the data set from replay buffer\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2,\n",
        "    num_parallel_calls=3).prefetch(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8frktk8JerL"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7VRZ1tCJerL"
      },
      "outputs": [],
      "source": [
        "# convert the main functions to TF functions\n",
        "\n",
        "collect_driver.run = function(collect_driver.run)\n",
        "agent.train = function(agent.train)\n",
        "\n",
        "def train_agent(n_iterations):\n",
        "    time_step = None\n",
        "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
        "    iterator = iter(dataset)\n",
        "    for iteration in range(n_iterations):\n",
        "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "        trajectories, buffer_info = next(iterator)\n",
        "        train_loss = agent.train(trajectories)\n",
        "        print(\"\\r{} loss:{:.5f}\".format(iteration, train_loss.loss.numpy()), end=\"\")\n",
        "        if iteration % 500 == 0:\n",
        "            logg=[]\n",
        "            for a in train_metrics:\n",
        "                logg = np.round(np.append(logg, np.asscalar(a.result().numpy())),2)\n",
        "            f=open('DQN.txt','ab')\n",
        "            np.savetxt(f, [logg], delimiter=\",\", fmt='%.1f')\n",
        "            f.close()\n",
        "            print(np.round(logg,1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "UzL4f07VJerM",
        "outputId": "54234228-a7e4-4fd6-f679-894574c07645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 loss:0.03892[  0. 296.   0.   0.]\n",
            "499 loss:0.00085"
          ]
        }
      ],
      "source": [
        "train_agent(n_iterations=500) # use 200000 or more here. 500 is just for tidy outcome"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X69XbSXMJerM"
      },
      "source": [
        "## Create visualisation with the trained deployment policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2jhW5I4JerM"
      },
      "outputs": [],
      "source": [
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLpZniu9JerM",
        "outputId": "4841a883-bcb0-4cbb-9821-9f9985c28949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000/1000"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.animation.FuncAnimation at 0x7f1cec6f39d0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "frames = []\n",
        "def save_frames(trajectory):\n",
        "    global frames\n",
        "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
        "\n",
        "prev_lives = tf_env.pyenv.envs[0].ale.lives()\n",
        "def reset_and_fire_on_life_lost(trajectory):\n",
        "    global prev_lives\n",
        "    lives = tf_env.pyenv.envs[0].ale.lives()\n",
        "    if prev_lives != lives:\n",
        "        tf_env.reset()\n",
        "        tf_env.pyenv.envs[0].step(1)\n",
        "        prev_lives = lives\n",
        "\n",
        "# access the deployment policy\n",
        "watch_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent.policy,\n",
        "    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(1000)],\n",
        "    num_steps=1000)\n",
        "\n",
        "final_time_step, final_policy_state = watch_driver.run()\n",
        "\n",
        "plot_animation(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emqFWFFcJerM"
      },
      "outputs": [],
      "source": [
        "# save the animated gif\n",
        "\n",
        "image_path = os.path.join(\"pong.gif\")\n",
        "frame_images = [PIL.Image.fromarray(frame) for frame in frames[:500]]\n",
        "frame_images[0].save(image_path, format='GIF',\n",
        "                     append_images=frame_images[1:],\n",
        "                     save_all=True,\n",
        "                     duration=60,\n",
        "                     loop=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE-2kywjJerR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "orig_nbformat": 2,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}